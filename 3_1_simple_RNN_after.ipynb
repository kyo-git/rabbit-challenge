{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyo-git/rabbit-challenge/blob/main/3_1_simple_RNN_after.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c3b146-3c19-46f7-e428-6fd5bcafbbb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/DNN_code_colab_ver200425')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ad1a1db-1f2f-4233-9296-96e9ab19a416"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.2183323144918663\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "65 + 51 = 188\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.131668925809919\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "22 + 48 = 201\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.1082240258252665\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "117 + 60 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0371911520994839\n",
            "Pred:[0 1 0 0 0 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "72 + 99 = 66\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9192269857460184\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "33 + 46 = 109\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0252992958097695\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "57 + 94 = 254\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.9700214614614796\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "98 + 7 = 91\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0727760411086051\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 0 0 1 1 0 1]\n",
            "12 + 1 = 255\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0743096272353507\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "48 + 126 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.9118674816267716\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "31 + 44 = 11\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0388719956927597\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "3 + 113 = 119\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.9066142793125389\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "93 + 16 = 255\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0332444881212968\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "92 + 126 = 160\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9066635839320977\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "82 + 124 = 254\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.0205025684091815\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "87 + 51 = 175\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9654213600038678\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "113 + 10 = 139\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.855791640794293\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 0 0 0 1 0]\n",
            "56 + 10 = 82\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.8341680414638024\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "28 + 33 = 127\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0529004658587944\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "52 + 113 = 109\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.7787011800402616\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "114 + 82 = 164\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.7106917679511888\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "5 + 34 = 47\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.1129972558344714\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "58 + 107 = 87\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.1742211770602389\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "111 + 87 = 191\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.7827060777002561\n",
            "Pred:[1 1 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "36 + 96 = 204\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.1249671764654574\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "111 + 20 = 123\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.9399335904520872\n",
            "Pred:[0 0 0 1 1 0 1 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "37 + 15 = 26\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.9463597307495792\n",
            "Pred:[0 0 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "61 + 3 = 6\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.9110227878625445\n",
            "Pred:[1 1 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "54 + 101 = 217\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.1329238083088975\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "59 + 95 = 254\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.5531969141336945\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "93 + 17 = 238\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.46432809510893913\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "86 + 70 = 152\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.6277333769174976\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "73 + 115 = 190\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.36910407685145874\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "6 + 98 = 104\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.6419643685680168\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "106 + 92 = 150\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.3012513530358695\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "84 + 6 = 90\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.5281098384973398\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "103 + 97 = 136\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.5682493622315842\n",
            "Pred:[1 1 1 0 1 1 0 1]\n",
            "True:[1 1 1 0 1 1 1 1]\n",
            "115 + 124 = 237\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.628692142793474\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "37 + 80 = 119\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.5419106136133951\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "117 + 35 = 158\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.3455052425841188\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "103 + 81 = 184\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.39089785823852846\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "1 + 114 = 115\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.19251647841624736\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "32 + 81 = 113\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.05563399303990273\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "70 + 56 = 126\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.26505513971271677\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "88 + 47 = 135\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.3650151834226536\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "25 + 116 = 205\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.10537245965875665\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "49 + 104 = 153\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.0997852173232299\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "46 + 34 = 80\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.060168936878660075\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "10 + 88 = 98\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.133830397033573\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "62 + 41 = 103\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.14453820379515234\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "9 + 104 = 113\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.0387549962834886\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "27 + 49 = 76\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.023564267066908128\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "53 + 121 = 174\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.061425393897801064\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "97 + 43 = 140\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.048959868060718975\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "106 + 53 = 159\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.06629557498563309\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "120 + 19 = 139\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.06069220420416574\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "27 + 95 = 122\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.04293310747079462\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "87 + 42 = 129\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.019934409930544163\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "107 + 114 = 221\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.03631053845084748\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "53 + 48 = 101\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.024528954469474954\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "108 + 45 = 153\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.026261189837457413\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "102 + 48 = 150\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.009120047874971685\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "105 + 57 = 162\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.012650816263064888\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "59 + 114 = 173\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.019860937039566064\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "76 + 3 = 79\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.01763228489127712\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "6 + 13 = 19\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0041838528816917075\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "85 + 101 = 186\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.01507053507095364\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "112 + 114 = 226\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.014212106524200161\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "94 + 48 = 142\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.02280681485669574\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "19 + 119 = 138\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.012660488860093336\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "22 + 47 = 69\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.017867956472096505\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "32 + 111 = 143\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.010744179490104575\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "32 + 7 = 39\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.014476793309389738\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "24 + 115 = 139\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.010847293579645888\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "100 + 20 = 120\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0014135607643582853\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 1 1 1 1 0]\n",
            "33 + 29 = 62\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0011764934241251502\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "1 + 97 = 98\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.00434300251058865\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "41 + 3 = 44\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.018597791981065995\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "66 + 30 = 96\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.005117539069240374\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "73 + 66 = 139\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.006937644635766766\n",
            "Pred:[1 1 1 0 0 1 1 1]\n",
            "True:[1 1 1 0 0 1 1 1]\n",
            "107 + 124 = 231\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.007321403278870846\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "38 + 59 = 97\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0032160651395045326\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "27 + 34 = 61\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.00681373913329619\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "44 + 14 = 58\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.006334790114062136\n",
            "Pred:[1 0 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "101 + 74 = 175\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0033432374052625815\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "51 + 72 = 123\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0011421099601780062\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "53 + 101 = 154\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0065776792213169365\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "108 + 20 = 128\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0037392086585651576\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "75 + 109 = 184\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.004914543722081153\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "18 + 106 = 124\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.003037594084636951\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "104 + 117 = 221\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0025427596391996133\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[1 1 1 0 1 1 0 0]\n",
            "119 + 117 = 236\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.001900686456497205\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "7 + 74 = 81\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.004653083543640344\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "42 + 34 = 76\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0038304954015905\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "56 + 59 = 115\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.001977008053412113\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "111 + 16 = 127\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0014867550373320801\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "17 + 59 = 76\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0014237561156834927\n",
            "Pred:[0 0 1 0 1 1 0 1]\n",
            "True:[0 0 1 0 1 1 0 1]\n",
            "42 + 3 = 45\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0016530255009666573\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "74 + 33 = 107\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0018904152161499058\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "44 + 41 = 85\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0031974172325940054\n",
            "Pred:[1 0 1 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "40 + 127 = 167\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}