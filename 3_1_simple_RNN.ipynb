{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyo-git/rabbit-challenge/blob/main/3_1_simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abec7b6-d95b-4520-faf2-77aef2a0ef64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/DNN_code_colab_ver200425')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa7a4a70-5de8-4bd2-b741-2f9ed95df90e"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# def d_tanh(x):\n",
        "\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "\n",
        "\n",
        "# He\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "\n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.2231441125427807\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "98 + 119 = 16\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0404077638199563\n",
            "Pred:[0 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "120 + 34 = 6\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9660829815369928\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "74 + 47 = 255\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9793290493923577\n",
            "Pred:[0 0 0 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "56 + 43 = 6\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0592032346497093\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "14 + 85 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.999436352724066\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "117 + 97 = 255\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0408965195729156\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "115 + 99 = 34\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.939374611038974\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "69 + 103 = 171\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0758940000357706\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "85 + 32 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0095782820221824\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "103 + 42 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.9719878608819559\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "23 + 28 = 4\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0621117465123109\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "86 + 34 = 4\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9451163095041337\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "78 + 102 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.8053731139656687\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "0 + 120 = 88\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8539245910782778\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "25 + 67 = 88\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8517761898943789\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "48 + 119 = 239\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.8367949971895415\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "2 + 48 = 116\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.0349509834334762\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "116 + 39 = 127\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9371707878655021\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "120 + 52 = 124\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.057263478773145\n",
            "Pred:[1 1 1 1 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "9 + 120 = 241\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9388631594569251\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 1 0 0 1 1 1 1]\n",
            "89 + 118 = 156\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.8475243937899638\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "2 + 111 = 253\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.8032910053227154\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "99 + 96 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.4631550684257181\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "84 + 68 = 152\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.9802165918550112\n",
            "Pred:[1 1 0 0 0 1 0 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "111 + 123 = 196\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.7930728015101678\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "100 + 48 = 80\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.40737811794594897\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "36 + 14 = 50\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.8072479636641436\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "110 + 42 = 92\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.3858244664444211\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "91 + 30 = 121\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.5016497343332399\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "45 + 34 = 79\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.27444934867642234\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "81 + 84 = 165\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.15567821998873102\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "64 + 66 = 130\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.13549933081885052\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "80 + 25 = 105\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.603068877916414\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "111 + 39 = 222\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.2436122398387382\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "1 + 18 = 19\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.15820034388807339\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "41 + 18 = 59\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.04416895747854034\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "72 + 83 = 155\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.11541235141409098\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "10 + 100 = 110\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.1450727740641818\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "74 + 90 = 164\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.11191778384507607\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "58 + 126 = 184\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.1310663456944107\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "86 + 6 = 92\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.08177678233327328\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "118 + 3 = 121\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.07761042164736813\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "38 + 120 = 158\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.10021524775177071\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "23 + 120 = 143\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.13985803333683397\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "110 + 24 = 134\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.050536201318030466\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "68 + 2 = 70\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.03549947083423382\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "76 + 51 = 127\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.05887589768686455\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "126 + 65 = 191\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.025238533578572926\n",
            "Pred:[1 1 0 0 0 1 0 1]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "102 + 95 = 197\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.05165804421523136\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "37 + 102 = 139\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.015278121286245202\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "107 + 19 = 126\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.04861562963854159\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "58 + 116 = 174\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.060702964752555225\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "119 + 74 = 193\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.011787819088175737\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "19 + 39 = 58\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.011582599905816754\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "99 + 17 = 116\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.05504361375526505\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "93 + 2 = 95\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.03906108021386043\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "26 + 12 = 38\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.0147468467484093\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "126 + 93 = 219\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.03566399014117679\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "25 + 112 = 137\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.022686250122685724\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "6 + 97 = 103\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.01936119390497336\n",
            "Pred:[1 0 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "121 + 70 = 191\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.020765509181489138\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "49 + 16 = 65\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.013223605950069066\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "63 + 117 = 180\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.006893670648861673\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "18 + 51 = 69\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.005388293933744799\n",
            "Pred:[1 1 0 1 0 0 0 1]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "100 + 109 = 209\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0038245890169101206\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "28 + 115 = 143\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.01585056532146412\n",
            "Pred:[1 1 1 0 1 0 1 1]\n",
            "True:[1 1 1 0 1 0 1 1]\n",
            "125 + 110 = 235\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.01702488951856875\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "68 + 50 = 118\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.010793677074389852\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "27 + 21 = 48\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.006309122909998031\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "94 + 105 = 199\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.003270877982549754\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "4 + 127 = 131\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.0030622750873052253\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "49 + 95 = 144\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0010307106083668224\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "75 + 127 = 202\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.009928391896183957\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "75 + 4 = 79\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.012221602723065772\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "87 + 106 = 193\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.009369580757260553\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "45 + 108 = 153\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.007976185263359598\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "23 + 40 = 63\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0024365695541440527\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "61 + 47 = 108\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0028703233359658\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "75 + 73 = 148\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.004105664737672281\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "50 + 15 = 65\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.009601884746484822\n",
            "Pred:[1 1 1 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "122 + 102 = 224\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.011141977345623357\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "39 + 66 = 105\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.011257889100232028\n",
            "Pred:[0 0 1 1 0 0 1 1]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "51 + 0 = 51\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.006041748908125629\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 1 0 1 1 1 1 0]\n",
            "116 + 106 = 222\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0013999460705914517\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "68 + 35 = 103\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.012485136914439598\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "127 + 14 = 141\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0012741301864576888\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "0 + 121 = 121\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0015305180836138945\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "48 + 63 = 111\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.005950800904906696\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "90 + 2 = 92\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.001173652367501514\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "27 + 11 = 38\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.005766043258424158\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "4 + 100 = 104\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.005160720404078759\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "48 + 24 = 72\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.005016386904803769\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "84 + 32 = 116\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.004523929283942819\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "114 + 112 = 226\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.005575147588461835\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "43 + 78 = 121\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005515488357988493\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "73 + 23 = 96\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0003652340746480269\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "25 + 49 = 74\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.002030822337618879\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "84 + 45 = 129\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0012792747399199898\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "40 + 109 = 149\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.0003070297812705215\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "33 + 105 = 138\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}